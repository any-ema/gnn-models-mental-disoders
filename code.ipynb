{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqOiZsUzBfOy"
      },
      "outputs": [],
      "source": [
        "!pip install torch-geometric\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torch_geometric.nn import GATConv, TransformerConv, GCNConv\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.data import Data, Batch\n",
        "from sklearn.model_selection import KFold\n",
        "import copy"
      ],
      "metadata": {
        "id": "JNShfE93BgKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "OYq-Nv7mBh9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "whB47S8jBjrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_edges(edge_file, node_names):\n",
        "    \"\"\"\n",
        "    Lê file de arestas. Suporta:\n",
        "    - matriz de adjacência CSV (linhas e colunas com nomes de nós) -> retorna directed edges onde weight != 0\n",
        "    - edge list com colunas (source, target, weight)\n",
        "    Retorna edge_index (2 x E) e edge_weight (E)\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(edge_file, index_col=0)\n",
        "    # Se df tem colunas iguais a node_names -> adjacency\n",
        "    if set(df.columns) >= set(node_names):\n",
        "        # adjacency matrix\n",
        "        mat = df.loc[node_names, node_names].values.astype(float)\n",
        "        src, tgt = np.where(~np.isclose(mat, 0.0))\n",
        "        weights = mat[src, tgt]\n",
        "        edge_index = torch.tensor(np.vstack((src, tgt)), dtype=torch.long)\n",
        "        edge_weight = torch.tensor(weights, dtype=torch.float)\n",
        "        return edge_index.to(DEVICE), edge_weight.to(DEVICE)\n",
        "    else:\n",
        "        # tentar edge list\n",
        "        df2 = pd.read_csv(edge_file)\n",
        "        # requer colunas source,target,weight\n",
        "        if set([\"source\",\"target\",\"weight\"]) <= set(df2.columns):\n",
        "            # map node names para índices\n",
        "            name_to_idx = {n:i for i,n in enumerate(node_names)}\n",
        "            src = [name_to_idx[s] for s in df2[\"source\"].values]\n",
        "            tgt = [name_to_idx[t] for t in df2[\"target\"].values]\n",
        "            weights = df2[\"weight\"].values.astype(float)\n",
        "            edge_index = torch.tensor([src, tgt], dtype=torch.long)\n",
        "            edge_weight = torch.tensor(weights, dtype=torch.float)\n",
        "            return edge_index.to(DEVICE), edge_weight.to(DEVICE)\n",
        "        else:\n",
        "            raise ValueError(\"Formato de edges não reconhecido. Forneça adjacency matrix (csv) ou edge list com colunas source,target,weight.\")\n"
      ],
      "metadata": {
        "id": "MsNyZH5UBlPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiPacienteTemporalDataset(Dataset):\n",
        "    def __init__(self, data_dict, seq_len=5, horizon=1):\n",
        "        self.samples = []\n",
        "        for pid, seq in data_dict.items():\n",
        "            for t in range(len(seq) - seq_len - horizon + 1):\n",
        "                X = seq[t:t+seq_len]       # [seq_len, N_nodes]\n",
        "                y = seq[t+seq_len:t+seq_len+horizon]  # [horizon, N_nodes]\n",
        "                self.samples.append((X, y))\n",
        "        self.seq_len = seq_len\n",
        "        self.horizon = horizon\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X, y = self.samples[idx]\n",
        "        return X.float(), y.float()"
      ],
      "metadata": {
        "id": "EegDOxEXBokj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X, y in train_loader:\n",
        "        X = X.unsqueeze(-1).to(DEVICE)  # [batch, seq_len, num_nodes, 1]\n",
        "        y = y[:, -1, :].unsqueeze(-1).to(DEVICE)  # [batch, num_nodes, 1]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X)\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in val_loader:\n",
        "            X = X.unsqueeze(-1).to(DEVICE)\n",
        "            y = y[:, -1, :].unsqueeze(-1).to(DEVICE)\n",
        "            output = model(X)\n",
        "            loss = criterion(output, y)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=100, patience=10, printepochs=True):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
        "        val_loss = validate_epoch(model, val_loader, criterion)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        if printepochs and epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}')\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            best_model_state = copy.deepcopy(model.state_dict())\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch}')\n",
        "            break\n",
        "\n",
        "    if best_model_state:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    return model, train_losses, val_losses\n",
        "\n",
        "def train_with_cross_validation(model_class, dataset, node_names, scalers=None,\n",
        "                                n_splits=5, test_size=0.2,\n",
        "                                num_epochs=100, patience=10, batch_size=32):\n",
        "    \"\"\"\n",
        "    Executa validação cruzada mantendo conjunto de teste fixo\n",
        "    \"\"\"\n",
        "    # Primeiro, separar conjunto de teste (fixo)\n",
        "    n_total = len(dataset)\n",
        "    n_test = max(1, int(test_size * n_total))  # Garantir pelo menos 1 amostra de teste\n",
        "    n_train_val = n_total - n_test\n",
        "\n",
        "    # Criar índice para split fixo\n",
        "    indices = list(range(n_total))\n",
        "    np.random.seed(42)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    test_idx = indices[:n_test]\n",
        "    train_val_idx = indices[n_test:]\n",
        "\n",
        "    test_subset = Subset(dataset, test_idx)\n",
        "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Ajustar n_splits se necessário\n",
        "    n_splits = min(n_splits, n_train_val)  # Não pode ter mais splits que amostras\n",
        "\n",
        "    if n_splits < 2:\n",
        "        print(\"Aviso: dados insuficientes para validação cruzada. Usando holdout simples.\")\n",
        "        # Treinar com todos os dados de treino/validação\n",
        "        train_subset = Subset(dataset, train_val_idx)\n",
        "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # Usar o mesmo conjunto para validação (não ideal, mas funciona)\n",
        "        val_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # Inicializar e treinar modelo\n",
        "        model = copy.deepcopy(model_class).to(DEVICE)\n",
        "        trained_model, train_losses, val_losses = train_model(\n",
        "            model, train_loader, val_loader, num_epochs, patience\n",
        "        )\n",
        "\n",
        "        fold_results = [{\n",
        "            'fold': 0,\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'best_val_loss': min(val_losses) if val_losses else np.inf,\n",
        "            'final_val_loss': val_losses[-1] if val_losses else np.inf\n",
        "        }]\n",
        "\n",
        "        best_model = trained_model\n",
        "    else:\n",
        "        # K-Fold apenas nos dados de treino/validação\n",
        "        kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "        fold_results = []\n",
        "        cv_models = []\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(kfold.split(train_val_idx)):\n",
        "            print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            # Mapear índices de volta para o dataset original\n",
        "            train_indices_fold = [train_val_idx[i] for i in train_idx]\n",
        "            val_indices_fold = [train_val_idx[i] for i in val_idx]\n",
        "\n",
        "            # Criar subsets\n",
        "            train_subset = Subset(dataset, train_indices_fold)\n",
        "            val_subset = Subset(dataset, val_indices_fold)\n",
        "\n",
        "            # Criar loaders\n",
        "            train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "            val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            # Inicializar modelo\n",
        "            model = copy.deepcopy(model_class).to(DEVICE)\n",
        "\n",
        "            # Treinar\n",
        "            trained_model, train_losses, val_losses = train_model(\n",
        "                model, train_loader, val_loader, num_epochs, patience\n",
        "            )\n",
        "\n",
        "            # Avaliar no fold de validação\n",
        "            trained_model.eval()\n",
        "            val_loss = 0\n",
        "            criterion = nn.MSELoss()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for X, y in val_loader:\n",
        "                    X = X.unsqueeze(-1).to(DEVICE)\n",
        "                    y = y[:, -1, :].unsqueeze(-1).to(DEVICE)\n",
        "                    val_loss += criterion(trained_model(X), y).item()\n",
        "\n",
        "            val_loss /= len(val_loader)\n",
        "\n",
        "            fold_results.append({\n",
        "                'fold': fold,\n",
        "                'train_losses': train_losses,\n",
        "                'val_losses': val_losses,\n",
        "                'best_val_loss': min(val_losses) if val_losses else np.inf,\n",
        "                'final_val_loss': val_loss\n",
        "            })\n",
        "\n",
        "            cv_models.append(trained_model)\n",
        "\n",
        "        # Encontrar o melhor modelo\n",
        "        best_fold = np.argmin([res['best_val_loss'] for res in fold_results])\n",
        "        best_model = cv_models[best_fold]\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Tamanho do dataset: {n_total} amostras\")\n",
        "    print(f\"Amostras de treino/validação: {n_train_val}\")\n",
        "    print(f\"Amostras de teste: {n_test}\")\n",
        "    print(f\"Número de folds: {n_splits}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Avaliar no conjunto de teste\n",
        "    evaluate_model(best_model, test_loader, node_names, scalers)\n",
        "\n",
        "    return fold_results, best_model, test_loader\n",
        "\n",
        "def evaluate_model(model, test_loader, node_names, scalers=None):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_loader:\n",
        "            X = X.unsqueeze(-1).to(DEVICE)\n",
        "            y = y[:, -1, :].cpu().numpy()\n",
        "            pred = model(X).cpu().numpy().squeeze(-1)\n",
        "            y_true.append(y)\n",
        "            y_pred.append(pred)\n",
        "\n",
        "    y_true = np.concatenate(y_true)\n",
        "    y_pred = np.concatenate(y_pred)\n",
        "\n",
        "    # --- se quiser reverter normalização ---\n",
        "    if scalers is not None:\n",
        "        for i, s in enumerate(node_names):\n",
        "            scaler = scalers[s]\n",
        "            y_true[:, i] = scaler.inverse_transform(y_true[:, i].reshape(-1, 1)).squeeze()\n",
        "            y_pred[:, i] = scaler.inverse_transform(y_pred[:, i].reshape(-1, 1)).squeeze()\n",
        "\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"\\nTEST - MAE: {mae:.4f} - RMSE: {rmse:.4f} - R²: {r2:.4f}\")\n",
        "    for i, name in enumerate(node_names):\n",
        "        mae_i = mean_absolute_error(y_true[:, i], y_pred[:, i])\n",
        "        r2_i = r2_score(y_true[:, i], y_pred[:, i])\n",
        "        print(f\"{name:15s} MAE: {mae_i:.4f}   R²: {r2_i:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "qPzsuqa3BqJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def normalize_patients(df, sintomas):\n",
        "    \"\"\"\n",
        "    Normaliza cada sintoma entre [0,1] globalmente (todos os pacientes).\n",
        "    Retorna df_normalizado e scalers para inversão posterior.\n",
        "    \"\"\"\n",
        "    scalers = {}\n",
        "    df_norm = df.copy()\n",
        "    for s in sintomas:\n",
        "        scaler = MinMaxScaler()\n",
        "        vals = df[[s]].values\n",
        "        df_norm[s] = scaler.fit_transform(vals)\n",
        "        scalers[s] = scaler\n",
        "    return df_norm, scalers"
      ],
      "metadata": {
        "id": "JGudU_k4Briz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def impute_timeseries(df_symptoms):\n",
        "    \"\"\"\n",
        "    df_symptoms: DataFrame com colunas de sintomas e time.\n",
        "    Faz interpolação linear por coluna e preenche bordas com média.\n",
        "    Retorna matriz (T x n_symptoms) com floats.\n",
        "    \"\"\"\n",
        "    X = df_symptoms.copy()\n",
        "    time_col = [c for c in X.columns if \"Time\" in c or \"mid\" in c]\n",
        "    # assume última coluna é tempo; remover antes de processar\n",
        "    if len(time_col)>0:\n",
        "        tcol = time_col[0]\n",
        "        Xvals = X.drop(columns=[tcol])\n",
        "    else:\n",
        "        Xvals = X\n",
        "\n",
        "    # substituir strings \"NA\" etc para NaN\n",
        "    Xvals = Xvals.replace([\"NA\",\"NaN\",\"nan\",\"\"], np.nan)\n",
        "    Xvals = Xvals.astype(float)\n",
        "\n",
        "    # interpolação temporal\n",
        "    Xvals = Xvals.interpolate(method='linear', limit_direction='both', axis=0)\n",
        "\n",
        "    # resto NaNs -> preencher com média da coluna\n",
        "    Xvals = Xvals.fillna(Xvals.mean())\n",
        "\n",
        "    return Xvals.values  # T x n_symptoms"
      ],
      "metadata": {
        "id": "ifyWHFd3BtWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('DataS1.TXT', sep=',')\n",
        "\n",
        "SINTOMAS = ['cheerful', 'pleasant_event', 'worry', 'fearful', 'sad', 'relaxed']\n",
        "\n",
        "column_map = {\n",
        "        'subjno': 'subject',\n",
        "        'dayno': 'day',\n",
        "        'beepno': 'beep',\n",
        "        'informat04': 'therapy',  # 0=control, 1=therapy\n",
        "        'st_period': 'period',    # 0=baseline, 1=post-baseline\n",
        "        'opgewkt_': 'cheerful',\n",
        "        'onplplez': 'pleasant_event',\n",
        "        'pieker': 'worry',\n",
        "        'angstig_': 'fearful',\n",
        "        'somber__': 'sad',\n",
        "        'ontspann': 'relaxed',\n",
        "        'neur': 'neuroticism'\n",
        "}\n",
        "\n",
        "df = df.rename(columns=column_map)\n",
        "\n",
        "df[SINTOMAS] = df[SINTOMAS].interpolate(method=\"linear\")\n",
        "\n",
        "df[SINTOMAS] = df[SINTOMAS].fillna(df.mean())"
      ],
      "metadata": {
        "id": "puocHWumBvBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('DataS1.TXT', sep=',')\n",
        "\n",
        "SINTOMAS = ['cheerful', 'pleasant_event', 'worry', 'fearful', 'sad', 'relaxed']\n",
        "\n",
        "column_map = {\n",
        "        'subjno': 'subject',\n",
        "        'dayno': 'day',\n",
        "        'beepno': 'beep',\n",
        "        'informat04': 'therapy',  # 0=control, 1=therapy\n",
        "        'st_period': 'period',    # 0=baseline, 1=post-baseline\n",
        "        'opgewkt_': 'cheerful',\n",
        "        'onplplez': 'pleasant_event',\n",
        "        'pieker': 'worry',\n",
        "        'angstig_': 'fearful',\n",
        "        'somber__': 'sad',\n",
        "        'ontspann': 'relaxed',\n",
        "        'neur': 'neuroticism'\n",
        "}\n",
        "\n",
        "df = df.rename(columns=column_map)\n",
        "\n",
        "df.loc[df['period'] == 1, 'subject'] = df.loc[df['period'] == 1, 'subject'] * 1000\n",
        "\n",
        "df[SINTOMAS] = df[SINTOMAS].interpolate(method=\"linear\")\n",
        "\n",
        "df_norm, scalers = normalize_patients(df, SINTOMAS)\n",
        "\n",
        "# preparar dados\n",
        "data_dict = {}\n",
        "for pid, df_sub in df_norm.groupby(\"subject\"):\n",
        "#     # Use impute_timeseries to handle NaNs consistently\n",
        "    seq = torch.tensor(impute_timeseries(df_sub[SINTOMAS]))\n",
        "    data_dict[pid] = seq\n",
        "\n",
        "edge_index, edge_weight = load_edges('coef_matrix.csv', SINTOMAS)\n",
        "\n",
        "# Define lookback window\n",
        "lookback_window = 5\n",
        "horizon = 1\n",
        "\n",
        "dataset = MultiPacienteTemporalDataset(data_dict, seq_len=lookback_window, horizon=horizon)"
      ],
      "metadata": {
        "id": "bB66sA7-B0P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpatialTemporalCGNModel(nn.Module):\n",
        "    def __init__(self, num_nodes, hidden_channels, num_gcn_layers, edge_index,\n",
        "                 edge_weight=None, device='cpu', dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_nodes = num_nodes\n",
        "        self.edge_index = edge_index\n",
        "        self.edge_weight = edge_weight\n",
        "        self.device = device\n",
        "        self.num_gcn_layers = num_gcn_layers\n",
        "\n",
        "        # Múltiplas camadas GCN\n",
        "        self.gcn_layers = nn.ModuleList()\n",
        "\n",
        "        # Primeira camada: 1 -> hidden_channels\n",
        "        self.gcn_layers.append(GCNConv(1, hidden_channels))\n",
        "\n",
        "        # Camadas intermediárias: hidden_channels -> hidden_channels\n",
        "        for _ in range(num_gcn_layers - 1):\n",
        "            self.gcn_layers.append(GCNConv(hidden_channels, hidden_channels))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(hidden_channels, hidden_channels, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_channels, 1)\n",
        "\n",
        "    def forward(self, X):\n",
        "        batch_size, seq_len, num_nodes, _ = X.shape\n",
        "        gcn_outs = []\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = X[:, t, :, :].reshape(batch_size * num_nodes, 1)\n",
        "\n",
        "            data_list = []\n",
        "            for b in range(batch_size):\n",
        "                x_nodes = x_t[b * num_nodes:(b+1) * num_nodes]\n",
        "                data_list.append(Data(x=x_nodes, edge_index=self.edge_index,\n",
        "                                     edge_attr=self.edge_weight))\n",
        "\n",
        "            batched_graph = Batch.from_data_list(data_list)\n",
        "\n",
        "            # Forward através de múltiplas camadas GCN\n",
        "            h = batched_graph.x\n",
        "            for i, gcn_layer in enumerate(self.gcn_layers):\n",
        "                h = gcn_layer(h, batched_graph.edge_index, batched_graph.edge_attr)\n",
        "                if i < len(self.gcn_layers) - 1:  # Não aplica dropout na última\n",
        "                    h = torch.relu(h)\n",
        "                    h = self.dropout(h)\n",
        "                else:\n",
        "                    h = torch.relu(h)  # Última camada só relu\n",
        "\n",
        "            h = h.view(batch_size, num_nodes, -1)\n",
        "            gcn_outs.append(h)\n",
        "\n",
        "        # Resto permanece igual...\n",
        "        gcn_seq = torch.stack(gcn_outs, dim=1)\n",
        "        gru_input = gcn_seq.permute(0,2,1,3).reshape(batch_size * num_nodes, seq_len, -1)\n",
        "        gru_out, _ = self.gru(gru_input)\n",
        "        last = gru_out[:, -1, :]\n",
        "        out = self.fc(last)\n",
        "        return out.view(batch_size, num_nodes, 1)"
      ],
      "metadata": {
        "id": "6SLmaP_QB2Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SpatialTemporalCGNModel(\n",
        "    num_nodes=len(SINTOMAS), # Pass num_nodes\n",
        "    hidden_channels=8, # Example hidden size\n",
        "    edge_index=edge_index,\n",
        "    edge_weight=edge_weight,\n",
        "    num_gcn_layers=8,\n",
        "    device= DEVICE\n",
        ").to(DEVICE)\n",
        "\n",
        "n_samples = len(dataset)\n",
        "n_splits = min(5, n_samples // 2)  # Ajuste automático baseado no tamanho do dataset\n",
        "\n",
        "fold_results, best_model, test_loader = train_with_cross_validation(\n",
        "    model_class=model,\n",
        "    dataset=dataset,\n",
        "    node_names=SINTOMAS,\n",
        "    scalers=scalers,\n",
        "    n_splits=n_splits,  # Ajustado automaticamente\n",
        "    test_size=0.2,\n",
        "    num_epochs=100,\n",
        "    patience=10,\n",
        "    batch_size=32\n",
        ")"
      ],
      "metadata": {
        "id": "4hKeF0NNB321"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalAttention(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, N, F]\n",
        "        B, T, N, F = x.shape\n",
        "\n",
        "        # Transpor para que cada nó veja sua sequência temporal\n",
        "        x_temp = x.transpose(1, 2)  # -> [B, N, T, F]\n",
        "\n",
        "        Q = self.query(x_temp)\n",
        "        K = self.key(x_temp)\n",
        "        V = self.value(x_temp)\n",
        "\n",
        "        # Atenção temporal (T x T)\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(F)\n",
        "        attn_weights = self.softmax(attn_scores)\n",
        "        temporal_out = torch.matmul(attn_weights, V)\n",
        "\n",
        "        return temporal_out.mean(dim=2)  # [B, N, F]\n"
      ],
      "metadata": {
        "id": "65Fh-RfhB8FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as Fnn\n",
        "\n",
        "\n",
        "class AttentiveTemporalGATGNN(nn.Module):\n",
        "    def __init__(self, num_nodes, in_channels=1, hidden_channels=32, out_channels=1,\n",
        "                 num_gat_layers=2,  # ✅ NOVO: número de camadas GAT\n",
        "                 edge_index=None, edge_attr=None, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.edge_index = edge_index\n",
        "        self.edge_attr = edge_attr\n",
        "        self.num_nodes = num_nodes\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.num_gat_layers = num_gat_layers\n",
        "\n",
        "        edge_dim = edge_attr.size(1) if edge_attr is not None else None\n",
        "\n",
        "        # ✅ Múltiplas camadas GAT\n",
        "        self.gat_layers = nn.ModuleList()\n",
        "        self.norm_layers = nn.ModuleList()\n",
        "\n",
        "        # Primeira camada\n",
        "        self.gat_layers.append(\n",
        "            GATConv(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=hidden_channels,\n",
        "                heads=2,\n",
        "                concat=False,\n",
        "                edge_dim=edge_dim,\n",
        "                dropout=dropout\n",
        "            )\n",
        "        )\n",
        "        self.norm_layers.append(nn.LayerNorm(hidden_channels))\n",
        "\n",
        "        # Camadas intermediárias\n",
        "        for _ in range(num_gat_layers - 1):\n",
        "            self.gat_layers.append(\n",
        "                GATConv(\n",
        "                    in_channels=hidden_channels,\n",
        "                    out_channels=hidden_channels,\n",
        "                    heads=2,\n",
        "                    concat=False,\n",
        "                    edge_dim=edge_dim,\n",
        "                    dropout=dropout\n",
        "                )\n",
        "            )\n",
        "            self.norm_layers.append(nn.LayerNorm(hidden_channels))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.temporal_attn = TemporalAttention(hidden_channels)\n",
        "        self.gru = nn.GRU(num_nodes * hidden_channels, hidden_channels, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, X_seq):\n",
        "        B, T, N, F = X_seq.size()\n",
        "        gat_outs = []\n",
        "\n",
        "        for t in range(T):\n",
        "            x_t = X_seq[:, t, :, :].reshape(B * N, F)\n",
        "\n",
        "            # Repete o grafo para cada batch\n",
        "            edge_indices, edge_attrs = [], []\n",
        "            for i in range(B):\n",
        "                edge_indices.append(self.edge_index + i * self.num_nodes)\n",
        "                if self.edge_attr is not None:\n",
        "                    edge_attrs.append(self.edge_attr)\n",
        "\n",
        "            batched_edge_index = torch.cat(edge_indices, dim=1)\n",
        "            batched_edge_attr = torch.cat(edge_attrs, dim=0) if edge_attrs else None\n",
        "\n",
        "            if batched_edge_attr is not None and batched_edge_attr.dim() == 1:\n",
        "                batched_edge_attr = batched_edge_attr.unsqueeze(-1)\n",
        "\n",
        "            # ✅ Forward através de múltiplas camadas GAT\n",
        "            for i, (gat_layer, norm_layer) in enumerate(zip(self.gat_layers, self.norm_layers)):\n",
        "                x_t = gat_layer(x_t, batched_edge_index, edge_attr=batched_edge_attr)\n",
        "\n",
        "                if i < len(self.gat_layers) - 1:  # Não na última camada\n",
        "                    x_t = norm_layer(x_t)\n",
        "                    x_t = Fnn.elu(x_t)  # ELU comum em GATs\n",
        "                    x_t = self.dropout(x_t)\n",
        "                else:\n",
        "                    x_t = Fnn.elu(x_t)  # Última camada só ELU\n",
        "\n",
        "            x_t = x_t.view(B, N, -1)\n",
        "            gat_outs.append(x_t)\n",
        "\n",
        "        H = torch.stack(gat_outs, dim=1)\n",
        "        attn_out = self.temporal_attn(H)\n",
        "\n",
        "        H_seq = H.view(B, T, self.num_nodes * self.hidden_channels)\n",
        "        _, h_n = self.gru(H_seq)\n",
        "        h_n = h_n.squeeze(0)\n",
        "\n",
        "        h_n_repeated = h_n.unsqueeze(1).repeat(1, N, 1)\n",
        "        combined = attn_out + h_n_repeated\n",
        "        out = self.linear(combined)\n",
        "        return out"
      ],
      "metadata": {
        "id": "G9Lx2N8bB937"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edge_attr = edge_weight.unsqueeze(-1)\n",
        "\n",
        "model = AttentiveTemporalGATGNN(\n",
        "    num_nodes=len(SINTOMAS), # Pass num_nodes\n",
        "    in_channels=1, # Feature dimension is 1\n",
        "    hidden_channels=8, # Example hidden size\n",
        "    out_channels=1, # Predicting one value per node\n",
        "    num_gat_layers=4,\n",
        "    edge_index=edge_index,\n",
        "    edge_attr=edge_attr\n",
        ").to(DEVICE)\n",
        "\n",
        "n_samples = len(dataset)\n",
        "n_splits = min(5, n_samples // 2)  # Ajuste automático baseado no tamanho do dataset\n",
        "\n",
        "fold_results, best_model, test_loader = train_with_cross_validation(\n",
        "    model_class=model,\n",
        "    dataset=dataset,\n",
        "    node_names=SINTOMAS,\n",
        "    scalers=scalers,\n",
        "    n_splits=n_splits,  # Ajustado automaticamente\n",
        "    test_size=0.2,\n",
        "    num_epochs=100,\n",
        "    patience=10,\n",
        "    batch_size=32\n",
        ")"
      ],
      "metadata": {
        "id": "FsD1bLNTB_Zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as Fnn\n",
        "\n",
        "class AttentiveTemporalTransformerGNN(nn.Module):\n",
        "    def __init__(self, num_nodes, in_channels=1, hidden_channels=32, out_channels=1,\n",
        "                 num_gnn_layers=2,  # ✅ NOVO: número de camadas GNN\n",
        "                 edge_index=None, edge_attr=None, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.edge_index = edge_index\n",
        "        self.edge_attr = edge_attr\n",
        "        self.num_nodes = num_nodes\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.num_gnn_layers = num_gnn_layers\n",
        "\n",
        "        # ✅ Múltiplas camadas TransformerConv\n",
        "        self.gnn_layers = nn.ModuleList()\n",
        "        self.norm_layers = nn.ModuleList()  # LayerNorm para estabilidade\n",
        "\n",
        "        # Primeira camada\n",
        "        self.gnn_layers.append(\n",
        "            TransformerConv(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=hidden_channels,\n",
        "                heads=2,\n",
        "                concat=False,\n",
        "                dropout=dropout,\n",
        "                edge_dim=1,\n",
        "                beta=True\n",
        "            )\n",
        "        )\n",
        "        self.norm_layers.append(nn.LayerNorm(hidden_channels))\n",
        "\n",
        "        # Camadas intermediárias\n",
        "        for _ in range(num_gnn_layers - 1):\n",
        "            self.gnn_layers.append(\n",
        "                TransformerConv(\n",
        "                    in_channels=hidden_channels,\n",
        "                    out_channels=hidden_channels,\n",
        "                    heads=2,\n",
        "                    concat=False,\n",
        "                    dropout=dropout,\n",
        "                    edge_dim=1,\n",
        "                    beta=True\n",
        "                )\n",
        "            )\n",
        "            self.norm_layers.append(nn.LayerNorm(hidden_channels))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.temporal_attn = TemporalAttention(hidden_channels)\n",
        "        self.gru = nn.GRU(num_nodes * hidden_channels, hidden_channels, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, X_seq):\n",
        "        B, T, N, F = X_seq.size()\n",
        "        gnn_outs = []\n",
        "\n",
        "        for t in range(T):\n",
        "            x_t = X_seq[:, t, :, :].reshape(B * N, F)\n",
        "\n",
        "            # Criar grafo batched\n",
        "            batched_edge_index, batched_edge_attr = self._create_batched_graph(B)\n",
        "\n",
        "            # ✅ Forward através de múltiplas camadas\n",
        "            h = x_t\n",
        "            for i, (gnn_layer, norm_layer) in enumerate(zip(self.gnn_layers, self.norm_layers)):\n",
        "                h = gnn_layer(h, batched_edge_index, edge_attr=batched_edge_attr)\n",
        "\n",
        "                if i < len(self.gnn_layers) - 1:  # Não na última camada\n",
        "                    h = norm_layer(h)\n",
        "                    h = Fnn.elu(h)  # ELU comum em Transformers\n",
        "                    h = self.dropout(h)\n",
        "                else:\n",
        "                    h = Fnn.elu(h)  # Última camada só ativação\n",
        "\n",
        "            h = h.view(B, N, -1)\n",
        "            gnn_outs.append(h)\n",
        "\n",
        "        H = torch.stack(gnn_outs, dim=1)\n",
        "        attn_out = self.temporal_attn(H)\n",
        "\n",
        "        H_seq = H.view(B, T, self.num_nodes * self.hidden_channels)\n",
        "        _, h_n = self.gru(H_seq)\n",
        "        h_n = h_n.squeeze(0)\n",
        "\n",
        "        h_n_repeated = h_n.unsqueeze(1).repeat(1, N, 1)\n",
        "        combined = attn_out + h_n_repeated\n",
        "        out = self.linear(combined)\n",
        "        return out\n",
        "\n",
        "    def _create_batched_graph(self, batch_size):\n",
        "        \"\"\"Helper para criar grafo batched\"\"\"\n",
        "        edge_indices = []\n",
        "        edge_attrs = []\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            edge_indices.append(self.edge_index + i * self.num_nodes)\n",
        "            if self.edge_attr is not None:\n",
        "                edge_attrs.append(self.edge_attr)\n",
        "\n",
        "        batched_edge_index = torch.cat(edge_indices, dim=1)\n",
        "        batched_edge_attr = torch.cat(edge_attrs, dim=0) if edge_attrs else None\n",
        "\n",
        "        if batched_edge_attr is not None and batched_edge_attr.dim() == 1:\n",
        "            batched_edge_attr = batched_edge_attr.unsqueeze(-1)\n",
        "\n",
        "        return batched_edge_index, batched_edge_attr"
      ],
      "metadata": {
        "id": "gpLEhvjHCCkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edge_attr = edge_weight.unsqueeze(-1)\n",
        "\n",
        "model = AttentiveTemporalTransformerGNN(\n",
        "    num_nodes=len(SINTOMAS), # Pass num_nodes\n",
        "    in_channels=1, # Feature dimension is 1\n",
        "    hidden_channels=8, # Example hidden size\n",
        "    out_channels=1, # Predicting one value per node\n",
        "    edge_index=edge_index,\n",
        "    edge_attr=edge_attr,\n",
        "    num_gnn_layers=4,\n",
        ").to(DEVICE)\n",
        "\n",
        "n_samples = len(dataset)\n",
        "n_splits = min(5, n_samples // 2)  # Ajuste automático baseado no tamanho do dataset\n",
        "\n",
        "fold_results, best_model, test_loader = train_with_cross_validation(\n",
        "    model_class=model,\n",
        "    dataset=dataset,\n",
        "    node_names=SINTOMAS,\n",
        "    scalers=scalers,\n",
        "    n_splits=n_splits,  # Ajustado automaticamente\n",
        "    test_size=0.2,\n",
        "    num_epochs=100,\n",
        "    patience=10,\n",
        "    batch_size=32\n",
        ")"
      ],
      "metadata": {
        "id": "eMlujD8ZCEM7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}